---
layout: post
title: "长长的旅程"
date: 2017-06-16
tag: 博客
---

### 长长的旅程

虽然深度学习人人都在喊人人都在用，但是它究竟为什么能有效果，到目前为止都没有初步的结论。具体来说，有两个理论问题没有解决，一个是泛化能力，另一个是收敛性。
深度神经网络那么多参数，再配以能拟合世界上大部分函数的强大模型容量，按通常机器学习理论的说法，应该是很容易过拟合才对；但是试看目前的卷积神经网络，往往只要是训练集上有了效果，
测试集上就不会太差。同样的参数下其它模型，比如说Random Forest和Adaboost，早就过拟合得不能看了。这是为什么呢？目前学术界没有统一的结论。退一步说，
假设我们已经承认了深度神经网络对某个问题有效，就是说存在一个权值的解使得期望误差很小，但因为深度学习优化是个非凸问题，这个存在的解，能否通过梯度下降的优化算法得到，
还是会陷入局部极小或者在鞍点停很久？这就牵涉到全局收敛性及收敛速度的问题。

这两个问题都很难很难，大家熟悉的工具在非线性传递函数及非凸问题面前似乎毫无用处，更不用说多层非线性网络带来的分析上的困难。为了能对付这个问题，需要去寻找更有效的数学工具，
使用更创造性的分析方法。

我这次终于做了第一篇关于深度学习理论的文章，投给了ICLR（链接见这里）。文章中首先对于单层ReLU下的梯度，在输入数据符合高斯分布的条件下给了一个闭式解，
然后用它证明了在某些适当的初始条件下，优化含有ReLU隐层的二层网络也可以收敛到指定解，而不会掉进局部极小，用的方法是常微分方程的一些理论。这个结论很有意思，
因为这类二层网络通常有指数量级的对称性，天生非凸，而要得到一个想要的解，就需要去研究不同初值下对称性破缺（Symmetry-Breaking）的结果。我在结果刚开始露头的时候和组
里人讨论过，各位大佬们都觉得玩对称性破缺这种东西非常非常难，我也没指望自己再往下挖能做出些什么来，不过在努力下，居然就得到了一些意想不到的结果。

我想着做层次式模型（Hierarchical model）及深度学习的理论已经有一些年了，从2010年开始一篇有关图像对齐的非凸问题的全局最优解拿了CVPR的Oral，
到PhD的中间几年开始想着怎么针对图像设计特定的层次式模型，之后在2013年，因为提出了CVPR那篇的层次式拓展，拿了ICCV13的Marr Prize Honorable Mention。
后来2013年9月进了谷歌还在业余时间继续做，险些投了一篇文章出去，可是因为无法将提出的理论框架和梯度下降算法结合起来而放弃；
最后在2015年时跳到Facebook人工智能研究所，终于有一些可以专心思考的时间，但就算如此仍然到现在才有一些结果，投了第一篇有关深度学习理论的文章。

我有时候觉得，要是不去想这些核心问题，而只集中精力于深度学习的应用的话，大概还可以提高两三倍的效率，文章会变多，人也会轻松得多，何乐而不为？然而或许是想惯了这个问题，
很难不去想现象背后的本质，结果背后的原因，就算是要忙其它的工作，自己有空时也在不停地思考。发《业余做研究的心得》系列，一方面是可以给大家分享一些心得体会，
另一方面也是不停地拷问自己，心得写了很多，可我做出了成果没有？

大半年前还在做围棋的时候，有位前辈在开会时问我，找个数据集画个网络图训练模型大家都会，作为一个研究员，你的核心技能是什么？我当时无言以对，心里虽早有答案，
可无法说出口。因为我知道，梦想在未成时一文不值。而让它变得有价值，是自己的责任。

现在回想起来，“失败是成功之母”并不对，“不历风雨如何见彩虹”也不对，因为喊着这些口号的时候，依然认为失败或者风雨是世上的稀罕事物，而成功则是要追求的目标。
殊不知这些观念，正是阻碍前进的最大原因。当失败到习以为常，当每时每刻都在风雨中穿行，当不再存有失败的概念，而只留下不停尝试的好奇心和不停总结的习惯，成功才可能悄然现身。
而伴随而来的，也不是那种梦寐以求的”我也终于牛了一次”的狂喜，而只是“啊，原来如此”的平静。

这篇文章，只是一个小小的开始；前面，还有很长很长的路要走。
